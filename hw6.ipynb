{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "close-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-benjamin",
   "metadata": {},
   "source": [
    "<b>Домашнее задание:</b>  \n",
    "\n",
    "Реализовать адаптивный бустинг использующий Логистическую Регрессию и меру ошибок LogLoss. Сравнить с точностью адаптивного бустинга на деревьях решений. Для сбора предсказаний можно использовать ту же функцию predict что и для бустинга на деревьях<br>  \n",
    "\n",
    "<i>Примечание: в LogLoss необходимо передавать не предсказания полученные с помощью clf.predict(...), а вероятность, полученную с помощью clf.predict_proba(...)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "insured-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, model_type='decision_tree'):\n",
    "        self.models = []\n",
    "        self.model_type = model_type\n",
    "        self.n_classes = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def log_loss(pred, y):\n",
    "        return -np.sum(y * np.log2(pred) + (1 - y) * np.log2(1 - pred)) / len(y)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_error(pred, y):\n",
    "        return sum(pred != y) / len(y)\n",
    "\n",
    "    def fit(self, X, y, n_models=50):\n",
    "        # Запишем количество классов в переменную\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        # Размер выборки\n",
    "        n_objects = len(X)\n",
    "\n",
    "        # Начальные веса деревьев\n",
    "        w = np.ones(n_objects) / n_objects\n",
    "\n",
    "        for n in range(n_models):\n",
    "            # Реализация на решающих пнях\n",
    "            if self.model_type == 'decision_tree':\n",
    "                clf = DecisionTreeClassifier(max_depth=1)\n",
    "                clf.fit(X, y, sample_weight=w)\n",
    "                predictions = clf.predict(X)\n",
    "                e = self.get_error(predictions, y)\n",
    "            # на логистической регрессии\n",
    "            elif self.model_type == 'logistic_regression':\n",
    "                clf = LogisticRegression(max_iter=1000)\n",
    "                clf.fit(X, y, sample_weight=w)\n",
    "                predictions = clf.predict_proba(X)[:, 1]\n",
    "                e = self.log_loss(predictions, y)\n",
    "            else:\n",
    "                raise ValueError('Wrong model type')\n",
    "\n",
    "            # отбросим дерево, если его ошибка больше 0.5\n",
    "            # Запишем условие в общем виде (применимо к небинарным классификаторам)\n",
    "            if e >= 1 - 1 / self.n_classes:\n",
    "                break\n",
    "\n",
    "            # Вычислим вес для дерева\n",
    "            alpha = 0.5 * np.log((1 - e) / e)\n",
    "\n",
    "            # Найдем индексы правильно классифицированных элементов\n",
    "            match = predictions == y\n",
    "\n",
    "            # Увеличим веса для неправильно классифицированных элементов\n",
    "            w[~match] *= np.exp(alpha)\n",
    "\n",
    "            # Нормализуем веса\n",
    "            w /= w.sum()\n",
    "\n",
    "            # Добавим дерево с весом в список\n",
    "            self.models.append((alpha, clf))\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_objects = len(X)\n",
    "\n",
    "        # вначале обозначим предсказание нулевым массивом\n",
    "        y_pred = np.zeros((n_objects, self.n_classes))\n",
    "\n",
    "        for alpha, clf in self.models:\n",
    "            prediction = clf.predict(X)\n",
    "            # Для каждого предсказания будем прибавлять alpha к\n",
    "            # элементу с индексом предсказанного класса\n",
    "            y_pred[range(n_objects), prediction] += alpha\n",
    "\n",
    "        # выберем индексы с максимальными суммарными весами -\n",
    "        # получим предсказанные алгоритмом классы\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def dec_predict(self, X):\n",
    "        n_objects = len(X)\n",
    "        score_bust = np.zeros(n_objects)\n",
    "\n",
    "        for alpha, clf in self.models:\n",
    "            score_i = clf.decision_function(X)\n",
    "            score_bust += score_i * alpha\n",
    "        y_pred = np.where(score_bust > 0, 1, 0)\n",
    "        return y_pred\n",
    "\n",
    "    def accuracy(self, X, y, decision=False):\n",
    "        if decision:\n",
    "            return (1 - self.get_error(self.dec_predict(X), y)) * 100\n",
    "        else:\n",
    "            return (1 - self.get_error(self.predict(X), y)) * 100\n",
    "\n",
    "    def return_error(self, X, y, decision=False):\n",
    "        if decision:\n",
    "            return self.get_error(self.dec_predict(X), y)\n",
    "        else:\n",
    "            return self.get_error(self.predict(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "qualified-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-insured",
   "metadata": {},
   "source": [
    "На пнях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "floating-safety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность алгоритма на обучающей выборке: 99.061\n",
      "Точность алгоритма на тестовой выборке: 94.406\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaBoost(model_type='decision_tree')\n",
    "adaboost.fit(X_train, y_train)\n",
    "print(f'Точность алгоритма на обучающей выборке: {adaboost.accuracy(X_train, y_train):.3f}')\n",
    "print(f'Точность алгоритма на тестовой выборке: {adaboost.accuracy(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-realtor",
   "metadata": {},
   "source": [
    "На логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "partial-madagascar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность алгоритма на обучающей выборке: 94.836\n",
      "Точность алгоритма на тестовой выборке: 93.706\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaBoost(model_type='logistic_regression')\n",
    "adaboost.fit(X_train, y_train)\n",
    "print(f'Точность алгоритма на обучающей выборке: {adaboost.accuracy(X_train, y_train):.3f}')\n",
    "print(f'Точность алгоритма на тестовой выборке: {adaboost.accuracy(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-mauritius",
   "metadata": {},
   "source": [
    "<b>Домашнее задание(необязательное, повышенной сложности):</b>  \n",
    "\n",
    "Реализовать специальную функцию predict для бустинга на логистической регрессии выводящую предсказания по формуле: $ Predictions=sign(Score_{bust}) $,\n",
    "где sign равен единице для положительных и нулю для отрицательных значений, а $ Score_{bust}= \\sum \\alpha_iScore_i$. Баллы выдаваемые каждой моделью $Score_i$ можно найти при помощи вызова метода decision_function на моделе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "attached-valve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность алгоритма на обучающей выборке: 94.836\n",
      "Точность алгоритма на тестовой выборке: 93.706\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaBoost(model_type='logistic_regression')\n",
    "adaboost.fit(X_train, y_train)\n",
    "print(f'Точность алгоритма на обучающей выборке: {adaboost.accuracy(X_train, y_train, decision=True):.3f}')\n",
    "print(f'Точность алгоритма на тестовой выборке: {adaboost.accuracy(X_test, y_test, decision=True):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
